defaults:
  - base: base
  - defaults: defaults
  - _self_

name: toy_dataset
batch_size: 4 # train batch size
eval_batch_size: 8 # evaluation batch size
num_workers: 10 # number of subprocesses to use for data loading
max_epochs: 800 # number of maximum epochs
use_amp: false # use AMP (Automatic Mixed Precision)

log_every_iters: 1
eval_every_epochs: 1
num_classes: 3 # this dataset has 3 classes (background, liver, tumor)
backend: gloo  # or nccl
engine_type: monai # or monai or ignite
roi_size: [64,64,8] # native roi size for model

model:
  type: monai # model type, torchvision or monai
  _target_: monai.networks.nets.SegResNet
  spatial_dims: 3 # number of spatial dimensions, 2 for 2D, 3 for 3D
  init_filters: 16 # initial number of filters
  in_channels: 1 # number of input channels, 1 for grayscale
  out_channels: ${num_classes} # number of output channels, should match num_classes -- is it including the background?
  dropout_prob: 0.0 # dropout probability, 0.2 is good default
  blocks_down: [1, 2, 2, 4] # default
  blocks_up: [1, 1, 1]  # somehow they do another upscaling after this
dataset:
  type: MonaiSegmentationFolder # dataset type
  images_dir: tests/test-data/liverv3_small/images # images path
  labels_dir: tests/test-data/liverv3_small/labels # labels path
  subset_size: null
  val_size: 0.2
  cache_num: 20 # number of samples to cache in memory
  transforms:  # only for monai datasets?
    train:  # NOTE: Order matters! Settings are based on: https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb
      - type: EnsureChannelFirst
        channel_dim: no_channel  # to automatically add channel dimension if not present
      - type: Orientation
        axcodes: 'RAS' # orientation codes for the image, RAS is standard for medical images
      - type: RandSpatialCrop
        # roi_size: [224, 224, 64] # crop size for training
        roi_size: ${roi_size} # crop size for training
        random_size: false # use fixed size for cropping
        random_center: false # use random center for cropping
      - type: NormalizeIntensity
        nonzero: true # normalize only non-zero values
        channel_wise: true # normalize each channel separately
        subtrahend: null # subtrahend for normalization
        divisor: null # divisor for normalization

loss:
  type: monai # loss type
  _target_: monai.losses.DiceLoss
  squared_pred: true
  to_onehot_y: true  # target is already one-hot encoded
  sigmoid: true
  include_background: true # include background class in the loss calculation

optimizer:
  type: SGD
  lr: 0.8 # base learning rate for the optimizer
  momentum: 0.2 # momentum for the optimizer
  weight_decay: 0 #5.0e-4 # weight decay for the optimizer
  nesterov: false # use Nesterov momentum

lr_scheduler:
  type: PolynomialLR # learning rate scheduler type
  power: 0.8 # power for the polynomial decay

handlers:
  train:  # Note: we copied handlers from base.yaml, otherwise they are overwritten (need to convert to e.g. dict? although order may not be preserved)
    - _target_: monai.handlers.CheckpointSaver
      _convert_: partial
      _requires_: [save_dict, save_dir]
      save_final: true
      final_filename: "final_model.pth"
      save_key_metric: false
      epoch_level: true
      save_interval: 5
      n_saved: 5
    - _target_: monai.handlers.ValidationHandler
      _requires_: validator
      interval: ${eval_every_epochs} # validate every epoch
      epoch_level: true # validate at the end of each epoch
      exec_at_start: true
    - _target_: monai.handlers.StatsHandler
      iteration_log: ${log_every_iters}
      epoch_log: false
      tag_name: "train loss"
    - _target_: monai.handlers.TensorBoardStatsHandler
      _requires_: [summary_writer]
      iteration_log: ${log_every_iters}
      epoch_log: true
      tag_name: "Train Loss"
    - _target_: ignite.handlers.ProgressBar
      desc: "Training"
    - _target_:  monai.handlers.LrScheduleHandler
      _requires_: lr_scheduler
      print_lr: true
      epoch_level: false
    # NOTE: ignite also offers a terminate on nan handler, could be interesting
      
